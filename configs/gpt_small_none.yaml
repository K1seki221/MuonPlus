optimizer_params:
  - name: muon-plus
    lr: [0.01]
    weight_decay: 0.1
    lr_schedule: constant-linear
    warm_up_fraction: 0.4
    ns_steps: 5
    polar_method: Keller
    rms_scaling: True
    # Split heads
    split_heads: False
    nheads: 12
    # Post-polar normalization
    # Options: none | col | row | col_row | row_col
    norm_mode: none
    norm_eps: 1.0e-8

training_params:
  tokens_processed: 1048576 #4194304 #4194304 # 2^18  # 524288 # 2^19
  val_tokens_processed: 8388608 #2^23
  batch_size: 8 # Could try also 64?
  num_epochs: 1
  context_length: 2048
  gradnorm: 1.0
  tensorcore_precision: high   #Can be highest, high, or medium
  autocast: True
  mixed_precision: bfloat16
  compile: True

logging_params:
  val_tokens_processed: 8388608 # 2^23
  log_step: 50
  val_step: 500
  save_ckpt_step: 500
  load_ckpt_step: 0
  keep_last: 2
  ckpt_dir: "outputs/checkpoints"
  results_dir: "outputs/results"
  wandb:
    project: "MuonPlus"
    dir: "outputs/wandb"

model_config:
  model_type: gpt
  block_size: 2048
  n_embd: 768
  n_layer: 12
  n_head: 12
  vocab_size: 50257
  flash_attention: True

dataset:
  name: "fineweb10B"
  tokenizer: "gpt2"
  total_tokens_B: 3
