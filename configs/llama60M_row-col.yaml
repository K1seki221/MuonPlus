# LLaMA 60M - muon-plus, row_col post-polar norm, cosine lr schedule
optimizer_params:
  - name: muon-plus
    lr: [0.04]
    weight_decay: 0.05
    lr_schedule: warm-up-cosine
    warm_up_fraction: 0.1
    ns_steps: 5
    polar_method: Keller
    rms_scaling: True
    # Split heads
    split_heads: False
    nheads: 8
    # Post-polar normalization
    # Options: none | col | row | col_row | row_col
    norm_mode: row_col
    norm_eps: 1.0e-8

training_params:
  tokens_processed: 524288
  val_tokens_processed: 8388608
  batch_size: 64
  num_epochs: 1
  context_length: 1024
  gradnorm: 1.0
  tensorcore_precision: high
  autocast: True
  mixed_precision: bfloat16
  compile: True

logging_params:
  val_tokens_processed: 8388608
  log_step: 50
  val_step: 500
  save_ckpt_step: 500
  load_ckpt_step: 0
  keep_last: 2
  ckpt_dir: "outputs/checkpoints"
  results_dir: "outputs/results"
  wandb:
    project: "MuonPlus"
    dir: "outputs/wandb"

# LLaMA 60M: ~60M params
# hidden_size=512, n_layer=8, n_head=8, intermediate_size=1376
model_config:
  model_type: llama
  n_embd: 512
  n_layer: 8
  n_head: 8
  vocab_size: 32000
  intermediate_size: 1376
  rope_theta: 10000.0
  flash_attention: True

dataset:
  name: "fineweb100B"
  tokenizer: "Llama-2-7b-hf"
  total_tokens_B: 1.1
